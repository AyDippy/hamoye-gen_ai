{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PLbDLM59KMK5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# installing the necessary libraries\n",
        "!pip install openai==v0.28.1\n",
        "!pip install --upgrade python-dotenv\n",
        "!pip install --upgrade langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#libraries needed\n",
        "import openai\n",
        "import IPython\n",
        "import os\n",
        "from dotenv import load_dotenv"
      ],
      "metadata": {
        "id": "cpiKZNEfKn5l"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_open_params(\n",
        "    model=\"gpt-3.5-turbo-instruct\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=256,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        "):\n",
        "    \"\"\"set openai parameter\"\"\"\n",
        "    openai_params = {}\n",
        "\n",
        "    openai_params['model'] = model\n",
        "    openai_params['temperature'] = temperature\n",
        "    openai_params['max_tokens'] = max_tokens\n",
        "    openai_params['top_p'] = top_p\n",
        "    openai_params['frequency_penalty'] = frequency_penalty\n",
        "    openai_params['presence_penalty'] = presence_penalty\n",
        "    return openai_params\n",
        "\n",
        "def get_completion(params, prompt):\n",
        "    \"\"\"Get completion from openai api\"\"\"\n",
        "    response = openai.Completion.create(\n",
        "        engine = params['model'],\n",
        "        prompt = prompt,\n",
        "        temperature = params['temperature'],\n",
        "        max_tokens = params['max_tokens'],\n",
        "        top_p = params['top_p'],\n",
        "        frequency_penalty = params['frequency_penalty'],\n",
        "        presence_penalty = params['presence_penalty'],\n",
        "    )\n",
        "    return response\n",
        "\n"
      ],
      "metadata": {
        "id": "M_StBzYjLnrx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "\n",
        "model=\"gpt-3.5-turbo\",\n",
        "\n",
        "messages=[\n",
        "\n",
        "{\n",
        "\n",
        "\"role\": \"system\",\n",
        "\n",
        "\"content\": \"You are a knowledgeable assistant. A user will ask a question and you should provide a clear and concise answer briefly enough to fit as an MCQ option.\"\n",
        "\n",
        "},\n",
        "\n",
        "{\n",
        "\n",
        "\"role\": \"user\",\n",
        "\n",
        "\"content\": \"What causes the Northern Lights?\"\n",
        "\n",
        "}\n",
        "\n",
        "],\n",
        "\n",
        "temperature=0,\n",
        "\n",
        "max_tokens=150,\n",
        "\n",
        "top_p=1\n",
        "\n",
        ")\n",
        "\n",
        "print(response.choices[0].message['content'])"
      ],
      "metadata": {
        "id": "KBkzaY-ML1Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",  # Note: This model might be unavailable, consider alternatives\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Provide a concise fact about the event in history mentioned by the user briefly enough to fit as an MCQ option.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What was the main cause of the American Civil War?\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.7,  # Increased for some creativity in fact selection\n",
        "    max_tokens=30,  # Limited for a concise fact\n",
        "    top_p=0.9  # Balanced informativeness and diversity\n",
        ")\n",
        "\n",
        "print(response.choices[0].message['content'])"
      ],
      "metadata": {
        "id": "NnhDv_jwL660"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "\n",
        "model=\"gpt-3.5-turbo\",\n",
        "\n",
        "messages=[\n",
        "\n",
        "{\n",
        "\n",
        "\"role\": \"system\",\n",
        "\n",
        "\"content\": \"You are to summarize the provided text into a concise form, maintaining the key points and context, briefly enough to fit as an MCQ option.\"\n",
        "\n",
        "},\n",
        "\n",
        "{\n",
        "\n",
        "\"role\": \"user\",\n",
        "\n",
        "\"content\": \"The economy has been growing steadily over the past few quarters, driven by increases in consumer spending and substantial investments in renewable energy infrastructure. Unemployment rates have also fallen to record lows as new industries are creating more jobs.\"\n",
        "\n",
        "}\n",
        "\n",
        "],\n",
        "\n",
        "temperature=0,\n",
        "\n",
        "max_tokens=100,\n",
        "\n",
        "top_p=1\n",
        "\n",
        ")\n",
        "\n",
        "print(response.choices[0].message['content'])"
      ],
      "metadata": {
        "id": "8uoz4MdnMiTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "\n",
        "model=\"gpt-3.5-turbo\",\n",
        "\n",
        "messages=[\n",
        "{\n",
        " \"role\": \"system\",\n",
        "\n",
        "\n",
        "\"content\": \"Translate the following sentence into Spanish concisely to fit as an MCQ option.\" # Insert system role and content here\n",
        "},\n",
        "{\n",
        "\n",
        "\"role\": \"user\",\n",
        "\n",
        "\"content\": \"What time is the meeting?\"\n",
        "\n",
        "}\n",
        "\n",
        "],\n",
        "\n",
        "temperature=0,\n",
        "\n",
        "max_tokens=30,\n",
        "\n",
        "top_p=1\n",
        "\n",
        ")\n",
        "\n",
        "print(response.choices[0].message['content'])"
      ],
      "metadata": {
        "id": "v5PDxPOVMwQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Classify the sentiment of the text provided by the user as positive, negative, or neutral.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"I had an awful experience at the restaurant. The food was bland and the service was slow.\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.7,  # Increased for some creativity\n",
        "    max_tokens=30,  # Increased to allow for a more complete response\n",
        "    top_p=0.9  # Adjusted for a balance between informativeness and diversity\n",
        ")\n",
        "\n",
        "print(response.choices[0].message['content'])"
      ],
      "metadata": {
        "id": "qB68-D0YM6OC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "\n",
        "model=\"gpt-3.5-turbo\",\n",
        "\n",
        "messages=[\n",
        "\n",
        "{\n",
        "\n",
        "\"role\": \"system\",\n",
        "\n",
        "\"content\": \"You will be provided with a sentence in English, and your task is to translate it into German.\"\n",
        "\n",
        "},\n",
        "\n",
        "{\n",
        "\n",
        "\"role\": \"user\",\n",
        "\n",
        "\"content\": \"How do you say 'I love to travel in German?\"\n",
        "\n",
        "}\n",
        "\n",
        "],\n",
        "\n",
        " temperature=0,\n",
        "\n",
        "max_tokens=64,\n",
        "\n",
        "top_p=1\n",
        "\n",
        ")\n",
        "\n",
        "print(response.choices[0].message['content'])"
      ],
      "metadata": {
        "id": "HCUwKMYNM_h2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"What is cloud computing?\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the concept of cloud computing briefly enough to fit as an MCQ option.\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.7,  # Increased for some creativity\n",
        "    max_tokens=60,  # Increased to allow for a more comprehensive explanation\n",
        "    top_p=0.9  # Adjusted for a balance between informativeness and diversity\n",
        ")\n",
        "\n",
        "print(response.choices[0].message['content'])"
      ],
      "metadata": {
        "id": "jwGv_OqeNnkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        " messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Who is known as the father of computer science?\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Provide a brief, accurate answer to the user’s question that can be used as an MCQ option.\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    max_tokens=30,\n",
        "    top_p=0.9\n",
        ")\n",
        "\n",
        "print(response.choices[0].message['content'])"
      ],
      "metadata": {
        "id": "wSrr3G4UNogu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Summarize the text in a concise form, highlighting key points about the economy.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"The economy has been growing steadily over the past few quarters, driven by increases in consumer spending and substantial investments in renewable energy infrastructure. Unemployment rates have also fallen to record lows as new industries are creating more jobs.\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.7,  # Increased for some creativity in phrasing\n",
        "    max_tokens=30,  # Reduced for a tighter MCQ option\n",
        "    top_p=0.9  # Balanced informativeness and diversity\n",
        ")\n",
        "\n",
        "print(response.choices[0].message['content'])\n",
        ""
      ],
      "metadata": {
        "id": "VJ-1YA8hNuEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4G_XNyMCN1Q_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}